{"cells":[{"metadata":{"_uuid":"78bb9ac640d6625586229d046c67a7c2461c5242","_cell_guid":"fd1f69c4-82bc-4d9f-9e8d-943100b19382","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D,LSTM\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\nfrom keras.layers.core import Reshape, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84556514b1f02ead3472578ae471e5fef0761f09","_cell_guid":"50771083-1694-4319-be99-2d41b83ac492","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/quora-record-linkage/quora_train.csv',index_col=0,usecols=[1,2,3,4,5,6]).dropna()\ntest_data = pd.read_csv('../input/quora-record-linkage/quora_test.csv',index_col=0,usecols=[1,2,3,4,5,6]).dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 7\nnp.random.seed(seed)\nX = train.iloc[:,2:4]\nY = train.iloc[:,4]\ntrain_data, val_data, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)\nX_test = test_data.iloc[:,2:4]\ny_test = test_data.iloc[:,4]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de2ae8c8e7548657bac8ece11ca928205779bd33","_cell_guid":"67810d98-bc8c-49ea-8285-55cf14c74100","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8a393afae2528f97f24fa1d49179340212582bf","_cell_guid":"73f85346-389c-481e-b182-07436c5004cc","trusted":true},"cell_type":"code","source":"NUM_WORDS=30522\ntokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                      lower=True)\ntokenizer.fit_on_texts(train_data.question1+' '+train_data.question2)\nsequences1_train = tokenizer.texts_to_sequences(train_data.question1)\nsequences2_train = tokenizer.texts_to_sequences(train_data.question2)\nsequences1_valid=tokenizer.texts_to_sequences(val_data.question1)\nsequences2_valid=tokenizer.texts_to_sequences(val_data.question2)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e453033d2b7514d2d184d623e5d1acbd537f9487","_cell_guid":"2af3b95b-1560-4a2e-89c4-921a31fb9120","trusted":true},"cell_type":"code","source":"X1_train = pad_sequences(sequences1_train,maxlen=32)\nX2_train = pad_sequences(sequences2_train,maxlen=32)\nX1_val = pad_sequences(sequences1_valid,maxlen=32)\nX2_val = pad_sequences(sequences2_valid,maxlen=32)\nprint('Shape of X train and X validation tensor:', X1_train.shape,X1_val.shape)\nprint('Shape of label train and validation tensor:', y_train.shape,y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!brew install wget\n\n!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6478a820872e903b7d5a3a8b0b3a543c773eb926","_cell_guid":"b5b196d3-a98e-4eb4-b731-c3f0650567ed","trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nword_vectors = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n\nEMBEDDING_DIM=300\nvocabulary_size=min(len(word_index)+1,NUM_WORDS)\nembedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i>=NUM_WORDS:\n        continue\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n\ndel(word_vectors)\n\nfrom keras.layers import Embedding\nembedding_layer = Embedding(vocabulary_size,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            trainable=False)\nsequence_length = X1_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lstm_model(embedding_layer,MAX_SEQUENCE_LENGTH):\n    lstm_layer = LSTM(256, dropout=0.1, recurrent_dropout=0.1)\n\n    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences_1 = embedding_layer(sequence_1_input)\n    x1 = lstm_layer(embedded_sequences_1)\n\n    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedded_sequences_2 = embedding_layer(sequence_2_input)\n    y1 = lstm_layer(embedded_sequences_2)\n\n    merged = concatenate([x1, y1])\n    merged = Dropout(0.1)(merged)\n    merged = BatchNormalization()(merged)\n\n    merged = Dense(128, activation='relu')(merged)\n    merged = Dropout(0.1)(merged)\n    merged = BatchNormalization()(merged)\n\n    preds = Dense(1, activation='sigmoid')(merged)\n    \n    model = Model(inputs=[sequence_1_input, sequence_2_input],outputs=preds)\n    adam = Adam(lr=1e-3)\n    model.compile(loss='binary_crossentropy',\n        optimizer=adam,\n        metrics=[tf.keras.metrics.AUC()])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_lstm_model(embedding_layer,sequence_length)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [EarlyStopping(monitor='val_loss')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([X1_train,X2_train], y_train, batch_size=1000, epochs=5, verbose=1, validation_data=([X1_val,X2_val],y_val),callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1ccc147faeb6eb331be53d3d70489cb24b5d294","_cell_guid":"aab8ced3-1224-4155-9d3d-f7a33e9575dc","trusted":true},"cell_type":"code","source":"sequences1_test=tokenizer.texts_to_sequences(test_data.question1)\nsequences2_test=tokenizer.texts_to_sequences(test_data.question2.astype(str))\nX1_test = pad_sequences(sequences1_test,maxlen=X1_train.shape[1])\nX2_test = pad_sequences(sequences2_test,maxlen=X1_train.shape[1])\ny_score=model.predict([X1_test,X2_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test=test_data.is_duplicate.values\nmodel.evaluate([X1_test,X2_test],y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision, recall, f1=precision_recall_curve(y_test,y_score)\naverage_precision = average_precision_score(y_test, y_score,average=\"micro\")\n\nplt.clf()\nplt.figure()\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x / (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append('iso-f1 curves')\nplt.step(recall,precision,where='post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    'Precision-Recall curve: AP={0:0.2f}'\n    .format(average_precision))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}